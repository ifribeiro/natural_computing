{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions\n",
    "import neural_network as nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocities(layers, n):\n",
    "    \"\"\"\n",
    "    Initialize the velocities\n",
    "    \"\"\"\n",
    "    velocities = functions.initialize_population(layers, n)\n",
    "    return velocities\n",
    "\n",
    "def update_particles_pos(particles, velocities):\n",
    "    \"\"\"\n",
    "    Update position of particles\n",
    "    \"\"\"\n",
    "    particles_ = list()\n",
    "    n_layers = len(particles[0])\n",
    "    for p,v in zip(particles, velocities):\n",
    "        p_layers = [p[i]+v[i] for i in range(n_layers)]\n",
    "        particles_.append(p_layers)\n",
    "    return particles_\n",
    "\n",
    "def calc_momentum(velocities, inertia):\n",
    "    \"\"\"\n",
    "    Calculate the mommentum of particles\n",
    "    \"\"\"\n",
    "    m = list()\n",
    "    n_layers = len(velocities[0])\n",
    "    for v in velocities:\n",
    "        m_layers = [v[i]*inertia for i in range(n_layers)]\n",
    "        m.append(m_layers)\n",
    "    return m\n",
    "\n",
    "def calc_accelerations(pa, ga, personal_bests, particles, global_best):\n",
    "    \"\"\"\n",
    "    Calculate the accelaration of particles\n",
    "    \"\"\"\n",
    "    acc_local = list()\n",
    "    acc_global = list()\n",
    "    for pbest, p in zip(personal_bests, particles):\n",
    "        l_layer = list()\n",
    "        g_layer = list()\n",
    "        # updates local and global velocities\n",
    "        for i in range(len(particles[0])):\n",
    "            shp = p[i].shape\n",
    "            li = (pbest[i]-p[i])*np.random.rand(shp[0], shp[1])*pa\n",
    "            gi = (global_best[i]-p[i])*np.random.rand(shp[0], shp[1])*ga\n",
    "            l_layer.append(li)\n",
    "            g_layer.append(gi)\n",
    "        \n",
    "        acc_local.append(l_layer)\n",
    "        acc_global.append(g_layer)\n",
    "        \n",
    "    return acc_local, acc_global\n",
    "\n",
    "def update_velocities(m, acc_local, acc_global):\n",
    "    \"\"\"\n",
    "    Update velocities of particles\n",
    "    \"\"\"\n",
    "    velocities = list()\n",
    "    for m_i, al_i, ag_i in zip(m, acc_local, acc_global):\n",
    "        vlist = list()\n",
    "        for i in range(len(m[0])):\n",
    "            v = m_i[i] + al_i[i] + ag_i[i]\n",
    "            vlist.append(v)\n",
    "        velocities.append(vlist)\n",
    "    return velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO_NN(layers=None, X_treino=None, X_test=None, y_train=None, y_test=None, encoder=None, swarm_size=20, inertia=0.5, pa=0.8, ga=0.9, num_iters=100, lograte=-1):\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization algorithm for training a Neural Network\n",
    "\n",
    "    Params:\n",
    "        - layers: NN architeture\n",
    "        - X_treino: train samples\n",
    "        - X_test: test samples\n",
    "        - y_train: labels for training samples\n",
    "        - y_test: labels for test samples\n",
    "        - encoder: encoder for labels\n",
    "        - swarm_size: size of the swarm\n",
    "        - inertia: inertia value for momentum\n",
    "        - pa: particle acceleration rate\n",
    "        - ga: global acclerationg rate\n",
    "        - num_iters: number of epochs\n",
    "        - lograte: rate to print logs\n",
    "\n",
    "    \"\"\"\n",
    "    # encoded labels for training and validation samples\n",
    "    y_true = encoder.inverse_transform(y_train).flatten()\n",
    "    y_true_v = encoder.inverse_transform(y_test).flatten()\n",
    "\n",
    "    # initialize particles and velocities\n",
    "    particles = functions.initialize_population(layers, swarm_size)\n",
    "    velocities = initialize_velocities(layers, swarm_size)\n",
    "    \n",
    "    # initialize personal and global bests\n",
    "    pbest = [p for p in particles]\n",
    "    pbest_loss = [np.inf for _ in particles]\n",
    "    pbest_loss_v = [np.inf for _ in particles]\n",
    "\n",
    "    gbest_idx = np.argmin(pbest_loss)\n",
    "    gbest = pbest[gbest_idx]\n",
    "    \n",
    "    # initialize fitness \n",
    "    gbest_loss, global_acc = functions.eval_individual(gbest, layers, X_treino, y_train,\n",
    "                                    y_true, loss='cce', encoder=encoder)\n",
    "    gbest_loss_v, global_acc_v = functions.eval_individual(gbest, layers, X_test, y_test,\n",
    "                                    y_true_v, loss='cce', encoder=encoder)\n",
    "    \n",
    "    loss_train = []\n",
    "    loss_vali = []\n",
    "    acc_train = []\n",
    "    acc_vali  = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # evaluate current swarm\n",
    "        for p_i in range(swarm_size):\n",
    "            fitness, acc = functions.eval_individual(particles[p_i], layers, X_treino, y_train, \n",
    "                                                    y_true, loss='cce', encoder=encoder)\n",
    "            fitness_v, acc_v = functions.eval_individual(particles[p_i], layers, X_test, y_test,\n",
    "                                                    y_true_v, loss='cce', encoder=encoder)\n",
    "            if fitness < pbest_loss[p_i]:\n",
    "                pbest[p_i] = particles[p_i]\n",
    "                pbest_loss[p_i] = fitness\n",
    "                pbest_loss_v[p_i] = fitness_v\n",
    "        \n",
    "        # global best\n",
    "        if np.min(pbest_loss) < gbest_loss:\n",
    "            gbest_idx = np.argmin(pbest_loss)\n",
    "            gbest = pbest[gbest_idx]\n",
    "            gbest_loss, global_acc = functions.eval_individual(gbest, layers, X_treino, y_train, y_true,\n",
    "                                        loss='cce', encoder=encoder)\n",
    "            \n",
    "            gbest_loss_v, global_acc_v = functions.eval_individual(gbest, layers, X_test, y_test, y_true_v,\n",
    "                                        loss='cce', encoder=encoder)\n",
    "\n",
    "        \n",
    "        # Calculate the momentum\n",
    "        m = calc_momentum(velocities, inertia)\n",
    "        # Calculate local and global accelerations\n",
    "        acc_local, acc_global = calc_accelerations(pa, ga, pbest, particles, gbest)\n",
    "        # Update the velocities\n",
    "        velocities = update_velocities(m, acc_local, acc_global)\n",
    "        # Update the position of particles\n",
    "        particles = update_particles_pos(particles, velocities)\n",
    "\n",
    "        loss_train.append(gbest_loss)\n",
    "        loss_vali.append(gbest_loss_v)\n",
    "        acc_train.append(global_acc)\n",
    "        acc_vali.append(global_acc_v)\n",
    "\n",
    "        if (i%lograte==0 and lograte>0):\n",
    "            print (\"#{} | loss_train:{:.2f}, loss_vali:{:.2f} | acc_train:{:.2f}, acc_vali:{:.2f}\".format(i, gbest_loss, gbest_loss_v ,global_acc, global_acc_v))\n",
    "    metrics = [loss_train, loss_vali, acc_train, acc_vali]\n",
    "    return gbest, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris, Y_iris = functions.load_dataset('iris')\n",
    "layers = [nn.Layer(4, 4, 'relu'), nn.Layer(4, 3, 'softmax')]\n",
    "args_iris = [X_iris, Y_iris, 150, 100, 0.9, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running exp 0\n",
      "Running exp 1\n",
      "Running exp 2\n",
      "Running exp 3\n",
      "Running exp 4\n",
      "Running exp 5\n",
      "Running exp 6\n",
      "Running exp 7\n",
      "Running exp 8\n",
      "Running exp 9\n",
      "Duration: 0:03:44.818398\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "functions.run_pso_experiments(PSO_NN, 10, layers, 'iris_pso', args=args_iris, norm=False)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine, y_wine = functions.load_dataset('wine')\n",
    "out_size = len(np.unique(y_wine))\n",
    "layers_wine = [nn.Layer(X_wine.shape[1], 4, 'relu'), nn.Layer(4, out_size, 'softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_wine= [X_wine, y_wine, 150, 100, 0.9, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running exp 0\n",
      "Running exp 1\n",
      "Running exp 2\n",
      "Running exp 3\n",
      "Running exp 4\n",
      "Running exp 5\n",
      "Running exp 6\n",
      "Running exp 7\n",
      "Running exp 8\n",
      "Running exp 9\n",
      "Duration: 0:03:34.177492\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "functions.run_pso_experiments(PSO_NN, 10, layers_wine, 'wine_pso', args=args_wine, norm=True)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_breast, y_breast = functions.load_dataset('breast')\n",
    "out_size = len(np.unique(y_breast))\n",
    "layers_breast = [nn.Layer(X_breast.shape[1], 4, 'relu'), nn.Layer(4, out_size, 'softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_breast= [X_breast, y_breast, 150, 100, 0.9, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running exp 0\n",
      "Running exp 1\n",
      "Running exp 2\n",
      "Running exp 3\n",
      "Running exp 4\n",
      "Running exp 5\n",
      "Running exp 6\n",
      "Running exp 7\n",
      "Running exp 8\n",
      "Running exp 9\n",
      "Duration: 0:07:21.452030\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "functions.run_pso_experiments(PSO_NN, 10, layers_breast, 'breast_pso', args=args_breast, norm=True)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
